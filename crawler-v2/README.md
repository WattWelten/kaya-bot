# KAYA Crawler v2

**Intelligente Daten-Extraktion fÃ¼r Landkreis Oldenburg**

## ğŸ¯ Ziel

Der KAYA Crawler v2 ist das HerzstÃ¼ck des KAYA-Systems. Er sammelt, verarbeitet und strukturiert alle relevanten Informationen fÃ¼r den Landkreis Oldenburg, um KAYA als perfekten digitalen Assistenten zu ermÃ¶glichen.

## ğŸ—ï¸ Architektur

```
crawler-v2/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/           # Haupt-Crawler-Engine
â”‚   â”œâ”€â”€ sources/         # Verschiedene Datenquellen
â”‚   â”œâ”€â”€ processors/      # Daten-Verarbeitung
â”‚   â”œâ”€â”€ config/          # Konfiguration
â”‚   â””â”€â”€ utils/           # Hilfsfunktionen
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/            # Rohe Daten
â”‚   â”œâ”€â”€ processed/      # Verarbeitete Daten
â”‚   â”œâ”€â”€ compressed/     # Komprimierte Daten
â”‚   â””â”€â”€ backup/         # Backup-Daten
â”œâ”€â”€ scripts/            # AusfÃ¼hrbare Scripts
â””â”€â”€ tests/             # Tests
```

## ğŸš€ Features

- **Multi-Source Crawling:** Web, Files, PDFs, APIs
- **Intelligente Verarbeitung:** Automatische Kategorisierung
- **Daten-Kompression:** Effiziente Speicherung
- **Backup-System:** Automatische Backups
- **Link-Validierung:** ÃœberprÃ¼fung der FunktionalitÃ¤t
- **Logging:** Umfassendes Logging-System

## ğŸ“¦ Installation

```bash
cd crawler-v2
npm install
```

## ğŸ”§ Verwendung

### VollstÃ¤ndiger Crawl
```bash
npm run crawl
```

### Test-Crawl
```bash
npm run test
```

### Einzelne Agent testen
```bash
node scripts/test.js
```

## ğŸ“Š Agent-Daten

Der Crawler sammelt Daten fÃ¼r folgende Agenten:

- **buergerdienste:** BÃ¼rgerdienste, Formulare, AntrÃ¤ge
- **ratsinfo:** Kreistag, Verwaltung
- **stellenportal:** Stellenausschreibungen
- **kontakte:** Ansprechpartner, Kontakte
- **jugend:** Jugendamt, Familienhilfe
- **soziales:** Sozialleistungen, Gesundheit

## ğŸ”„ Datenfluss

1. **Crawling:** Sammlung von Daten aus verschiedenen Quellen
2. **Verarbeitung:** Strukturierung und Bereinigung
3. **Kompression:** Effiziente Speicherung
4. **Backup:** Sicherung der Daten
5. **Integration:** Bereitstellung fÃ¼r KAYA

## ğŸ“ˆ Performance

- **Parallel Processing:** Mehrere Agenten gleichzeitig
- **Lazy Loading:** Nur benÃ¶tigte Daten laden
- **Caching:** Zwischenspeicherung fÃ¼r bessere Performance
- **Compression:** Reduzierung der SpeichergrÃ¶ÃŸe

## ğŸ› ï¸ Konfiguration

Die Crawler-Konfiguration kann in `src/config/` angepasst werden:

- **Agent-URLs:** Welche Websites gecrawlt werden
- **File-Paths:** Lokale Dateien und PDFs
- **Processing-Rules:** Wie Daten verarbeitet werden

## ğŸ“ Logging

Alle AktivitÃ¤ten werden in `logs/` protokolliert:

- **crawler.log:** Allgemeine AktivitÃ¤ten
- **error.log:** Fehler und Warnungen

## ğŸ”’ Sicherheit

- **Rate Limiting:** Schutz vor Ãœberlastung
- **User-Agent:** Identifikation als legitimer Bot
- **Timeout:** Schutz vor hÃ¤ngenden Requests
- **Error Handling:** Robuste Fehlerbehandlung

## ğŸ§ª Testing

```bash
npm test
```

## ğŸ“ Support

Bei Fragen oder Problemen wende dich an das KAYA-Team.

---

**KAYA Crawler v2 - Intelligente Daten-Extraktion fÃ¼r den Landkreis Oldenburg**

